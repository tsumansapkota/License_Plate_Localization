{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>points</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/dactar/1300_img.png</td>\n",
       "      <td>[177.16228095049055, 184.83182101741053, 244.7...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/dactar/1766_img.png</td>\n",
       "      <td>[409.7109787838158, 362.74050003943523, 536.55...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/dactar/130_img.png</td>\n",
       "      <td>[386.66894459252546, 342.1963080587535, 503.59...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/dactar/1785_img.png</td>\n",
       "      <td>[197.75577726950075, 171.98786970581278, 250.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/dactar/617_img.png</td>\n",
       "      <td>[180.14505945291972, 298.14972872833135, 288.5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      image                                             points\n",
       "0  data/dactar/1300_img.png  [177.16228095049055, 184.83182101741053, 244.7...\n",
       "1  data/dactar/1766_img.png  [409.7109787838158, 362.74050003943523, 536.55...\n",
       "2   data/dactar/130_img.png  [386.66894459252546, 342.1963080587535, 503.59...\n",
       "3  data/dactar/1785_img.png  [197.75577726950075, 171.98786970581278, 250.1...\n",
       "4   data/dactar/617_img.png  [180.14505945291972, 298.14972872833135, 288.5..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_pickle(\"data/dataset.pkl\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.optim import lr_scheduler\n",
    "from torchvision import datasets, models, transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "import cv2\n",
    "import progressbar\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_2d_tensor(inp):\n",
    "    inp = torch.Tensor(inp)\n",
    "    if len(inp.size()) < 2:\n",
    "        inp = inp.unsqueeze(0)\n",
    "    return inp\n",
    "\n",
    "def xywh_to_x1y1x2y2(boxes):\n",
    "    boxes = to_2d_tensor(boxes)\n",
    "    boxes[:, 2] += boxes[:, 0] - 1\n",
    "    boxes[:, 3] += boxes[:, 1] - 1\n",
    "    return boxes\n",
    "\n",
    "def x1y1x2y2_to_xywh(boxes):\n",
    "    boxes = to_2d_tensor(boxes)\n",
    "    boxes[:, 2] -= boxes[:, 0] - 1\n",
    "    boxes[:, 3] -= boxes[:, 1] - 1\n",
    "    return boxes\n",
    "\n",
    "def crop_boxes(boxes, im_sizes):\n",
    "    boxes = to_2d_tensor(boxes)\n",
    "    im_sizes = to_2d_tensor(im_sizes)\n",
    "    boxes = xywh_to_x1y1x2y2(boxes)\n",
    "    zero = torch.Tensor([0])\n",
    "    boxes[:, 0] = torch.max(torch.min(boxes[:, 0], im_sizes[:, 0]), zero)\n",
    "    boxes[:, 1] = torch.max(torch.min(boxes[:, 1], im_sizes[:, 1]), zero)\n",
    "    boxes[:, 2] = torch.max(torch.min(boxes[:, 2], im_sizes[:, 0]), zero)\n",
    "    boxes[:, 3] = torch.max(torch.min(boxes[:, 3], im_sizes[:, 1]), zero)\n",
    "    boxes = x1y1x2y2_to_xywh(boxes)\n",
    "    return boxes\n",
    "\n",
    "def box_transform(boxes, im_sizes):\n",
    "    # box in (x, y, w, h) format\n",
    "    boxes = to_2d_tensor(boxes)\n",
    "    im_sizes = to_2d_tensor(im_sizes)\n",
    "    boxes[:, 0] = 2 * boxes[:, 0] / im_sizes[:, 0] - 1\n",
    "    boxes[:, 1] = 2 * boxes[:, 1] / im_sizes[:, 1] - 1\n",
    "    boxes[:, 2] = 2 * boxes[:, 2] / im_sizes[:, 0]\n",
    "    boxes[:, 3] = 2 * boxes[:, 3] / im_sizes[:, 1]\n",
    "    return boxes\n",
    "\n",
    "def box_transform_inv(boxes, im_sizes):\n",
    "    # box in (x, y, w, h) format\n",
    "    boxes = to_2d_tensor(boxes)\n",
    "    im_sizes = to_2d_tensor(im_sizes)\n",
    "    boxes[:, 0] = (boxes[:, 0] + 1) / 2 * im_sizes[:, 0]\n",
    "    boxes[:, 1] = (boxes[:, 1] + 1) / 2 * im_sizes[:, 1]\n",
    "    boxes[:, 2] = boxes[:, 2] / 2 * im_sizes[:, 0]\n",
    "    boxes[:, 3] = boxes[:, 3] / 2 * im_sizes[:, 1]\n",
    "    return boxes\n",
    "\n",
    "def compute_IoU(boxes1, boxes2):\n",
    "    boxes1 = to_2d_tensor(boxes1)\n",
    "    boxes1 = xywh_to_x1y1x2y2(boxes1)\n",
    "    boxes2 = to_2d_tensor(boxes2)\n",
    "    boxes2 = xywh_to_x1y1x2y2(boxes2)\n",
    "    \n",
    "    intersec = boxes1.clone()\n",
    "    intersec[:, 0] = torch.max(boxes1[:, 0], boxes2[:, 0])\n",
    "    intersec[:, 1] = torch.max(boxes1[:, 1], boxes2[:, 1])\n",
    "    intersec[:, 2] = torch.min(boxes1[:, 2], boxes2[:, 2])\n",
    "    intersec[:, 3] = torch.min(boxes1[:, 3], boxes2[:, 3])\n",
    "    \n",
    "    def compute_area(boxes):\n",
    "        # in (x1, y1, x2, y2) format\n",
    "        dx = boxes[:, 2] - boxes[:, 0]\n",
    "        dx[dx < 0] = 0\n",
    "        dy = boxes[:, 3] - boxes[:, 1]\n",
    "        dy[dy < 0] = 0\n",
    "        return dx * dy\n",
    "    \n",
    "    a1 = compute_area(boxes1)\n",
    "    a2 = compute_area(boxes2)\n",
    "    ia = compute_area(intersec)\n",
    "    assert((a1 + a2 - ia <= 0).sum() == 0)\n",
    "    \n",
    "    return ia / (a1 + a2 - ia)    \n",
    "\n",
    "def compute_acc(preds, targets, im_sizes, theta=0.75):\n",
    "    preds = box_transform_inv(preds.clone(), im_sizes)\n",
    "    preds = crop_boxes(preds, im_sizes)\n",
    "    targets = box_transform_inv(targets.clone(), im_sizes)\n",
    "    IoU = compute_IoU(preds, targets)\n",
    "    corr = (IoU >= theta).sum()\n",
    "    return corr / preds.size(0)\n",
    "\n",
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.cnt = 0\n",
    "        \n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.cnt += n\n",
    "        self.avg = self.sum / self.cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResizeAspect(object):\n",
    "    def __init__(self, h, w):\n",
    "        self.rescale_factor=None\n",
    "        self.shift_w=None\n",
    "        self.shift_h=None\n",
    "        self.hw = (h, w)\n",
    "        \n",
    "    def do_image(self,img):\n",
    "        h, w = self.hw\n",
    "        img_h, img_w = img.shape[0], img.shape[1]\n",
    "        rescale_factor = min(w/img_w, h/img_h)\n",
    "        new_w = int(img_w * rescale_factor)\n",
    "        new_h = int(img_h * rescale_factor)\n",
    "        resized_image = cv2.resize(img, (new_w,new_h), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "        canvas = np.full((h, w, 3), 128, dtype=np.uint8)\n",
    "        shift_h = (h-new_h)//2\n",
    "        shift_w = (w-new_w)//2\n",
    "        canvas[(h-new_h)//2:(h-new_h)//2 + new_h,(w-new_w)//2:(w-new_w)//2 + new_w,  :] = resized_image\n",
    "        img = canvas.copy()\n",
    "        self.rescale_factor=rescale_factor\n",
    "        self.shift_h = shift_h\n",
    "        self.shift_w = shift_w\n",
    "        return img\n",
    "    \n",
    "    def do_box(self, box):\n",
    "#         if self.rescale_factor is None:\n",
    "#             print('The image is not scaled, do_image first!!')\n",
    "#             return\n",
    "        box = box.reshape(-1,2)\n",
    "        box *=self.rescale_factor\n",
    "        box[: ,0] += self.shift_w\n",
    "        box[: ,1] += self.shift_h\n",
    "        box = box.reshape(-1)\n",
    "        return box\n",
    "    \n",
    "    def undo_box(self, box):\n",
    "        box = box.reshape(-1,2)\n",
    "        box[: ,0] -= self.shift_w\n",
    "        box[: ,1] -= self.shift_h\n",
    "        box /=self.rescale_factor\n",
    "        box = box.reshape(-1)\n",
    "        return box\n",
    "    \n",
    "    \n",
    "# class ImageData(object):\n",
    "    \n",
    "#     def __init__(self, img_dir, box):\n",
    "#         self.img_dir = img_dir\n",
    "#         self.box = box\n",
    "#         self.img = None\n",
    "#         self._load_img()\n",
    "        \n",
    "#     def _load_img(self):\n",
    "#         self.img = Image.open(self.img_dir).convert('RGB')\n",
    "#         self.img = np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinalTransform:\n",
    "    def __init__(self):\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])])\n",
    "    \n",
    "    def transform_inv(self,img):\n",
    "        inp = img.numpy().transpose((1, 2, 0))\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        inp = std * inp + mean\n",
    "        inp = np.clip(inp, 0, 1)\n",
    "        return inp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LicenseDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        \n",
    "        self.imgs = list(df.image)\n",
    "        self.boxes = df.points.tolist()\n",
    "#         self.labeled = df.labeled.tolist()\n",
    "        self.final_transform = FinalTransform()\n",
    "        self.transform = self.final_transform.transform\n",
    "#         self.transform = transforms.Compose([\n",
    "# #                 transforms.Resize((224, 224)),\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "#                                  std=[0.229, 0.224, 0.225])\n",
    "#         ])\n",
    "        \n",
    "    def transform_inv(self,img):\n",
    "        inp = img.numpy().transpose((1, 2, 0))\n",
    "        mean = np.array([0.485, 0.456, 0.406])\n",
    "        std = np.array([0.229, 0.224, 0.225])\n",
    "        inp = std * inp + mean\n",
    "        inp = np.clip(inp, 0, 1)\n",
    "        return inp\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        path= self.imgs[index]\n",
    "        box = self.boxes[index]\n",
    "        \n",
    "        img = Image.open(path).convert('RGB')\n",
    "        resizer = ResizeAspect(h=224, w=224)\n",
    "        img = resizer.do_image(np.array(img))\n",
    "        img = self.final_transform.transform(img)\n",
    "        box = resizer.do_box(box)\n",
    "        box = np.array(box, dtype=np.float32)\n",
    "        \n",
    "        return img, box #, resizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(df, test_size=0.1):\n",
    "    if isinstance(test_size, float):\n",
    "        test_size = int(test_size * len(df))\n",
    "\n",
    "    indices = df.index.tolist()\n",
    "    test_indices = random.sample(population=indices, k=test_size)\n",
    "\n",
    "    test_df = df.loc[test_indices]\n",
    "    train_df = df.drop(test_indices)\n",
    "    return train_df, test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = LicenseDataset(train_df)\n",
    "test = LicenseDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ind = random.choice(range(len(train_df)))\n",
    "# img, box, img_size = train[ind]\n",
    "\n",
    "# box = np.append(box, box[:2]).reshape(-1,2)\n",
    "# img = train.final_transform.transform_inv(img)\n",
    "\n",
    "# plt.imshow(img)\n",
    "# plt.plot(box[:,0], box[:,1], lw=3, c='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "                train, batch_size=32,shuffle=True,\n",
    "                num_workers=2, pin_memory=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "                test, batch_size=32,shuffle=False,\n",
    "                num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "'''\n",
    "output of our model is :\n",
    "x1, y1,\n",
    "x2, y2,\n",
    "x3, y3,\n",
    "x4, y4,\n",
    "conf -> only when no bounding box images are taken\n",
    "'''\n",
    "num_feature = model.fc.in_features\n",
    "num_output = 8#9\n",
    "model.fc = nn.Linear(num_feature, num_output)\n",
    "# model.eval()\n",
    "# model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.SmoothL1Loss().cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_state = model.state_dict()\n",
    "best_epoch = -1\n",
    "best_acc = 0.0\n",
    "\n",
    "epoch_loss = {True: [], False: []}\n",
    "epoch_acc = {True: [], False: []}\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0% |                                                                        |\r"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type Float but got scalar type Double for argument #2 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-5dc65038ba74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;31m#             acc = compute_acc(outputs.data.cpu(), targets.data.cpu(), im_sizes)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Program_Files/miniconda3.7/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Program_Files/miniconda3.7/lib/python3.7/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    802\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Program_Files/miniconda3.7/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msmooth_l1_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   2203\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2204\u001b[0m         \u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbroadcast_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2205\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msmooth_l1_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpanded_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2206\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type Float but got scalar type Double for argument #2 'target'"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    accs = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    for train_mode in (True, False):\n",
    "        if train_mode:\n",
    "            scheduler.step()\n",
    "            data_loader = train_loader\n",
    "        else:\n",
    "            data_loader = test_loader\n",
    "        model.train(mode=train_mode)\n",
    "            \n",
    "        end = time.time()\n",
    "        bar = progressbar.ProgressBar()\n",
    "        for ims, boxes in bar(data_loader):\n",
    "            \n",
    "            inputs = ims\n",
    "            targets = boxes\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # forward\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "#             acc = compute_acc(outputs.data.cpu(), targets.data.cpu(), im_sizes)\n",
    "            \n",
    "            nsample = inputs.size(0)\n",
    "#             accs.update(acc, nsample)\n",
    "            losses.update(loss.data[0], nsample)\n",
    "            \n",
    "            if train_mode:\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "#         if not train_mode and accs.avg > best_acc:\n",
    "#             best_acc = accs.avg\n",
    "#             best_epoch = epoch\n",
    "#             best_model_state = model.state_dict()\n",
    "            \n",
    "        elapsed_time = time.time() - end\n",
    "        print('[{}]\\tEpoch: {}/{}\\tLoss: {:.4f}\\tAcc: {:.2%}\\tTime: {:.3f}'.format(\n",
    "            phase, epoch+1, epochs, losses.avg, accs.avg, elapsed_time))\n",
    "        epoch_loss[phase].append(losses.avg)\n",
    "        epoch_acc[phase].append(accs.avg)\n",
    "        \n",
    "    print('[Info] best test acc: {:.2%} at {}th epoch'.format(best_acc, best_epoch))\n",
    "    torch.save(best_model_state, 'best_model_state.path.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
